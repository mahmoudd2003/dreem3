# app.py
# ===========================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit Ù„Ù†Ø¸Ø§Ù… ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…
# ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰:
# - utils/openai_client.py    (generate_article + Two-pass + enforce_outline)
# - utils/exporters.py        (ØªØµØ¯ÙŠØ± Markdown/DOCX/JSON)
# - utils/quality_checks.py   (ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠ)
# - utils/meta_generator.py   (Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ÙŠØªØ§ Ø§Ù„Ø°ÙƒÙŠ)
# - utils/internal_links.py   (Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)
# - utils/style_diversity.py  (Ù…Ø¤Ø´Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨)
# - utils/section_tools.py    (ØªØ­Ø±ÙŠØ±/Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯)
# - utils/text_cleanup.py     (ÙÙ„ØªØ±Ø©/ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„ØºÙˆÙŠØ©)
# - utils/heading_tools.py    (Normalize Headings)
# - utils/enhanced_fix.py     (Ø¯ÙØ¹Ø© Ø¥ØµÙ„Ø§Ø­ Ù…ÙØ­Ø³Ù‘Ù†Ø©)
# ===========================

import re
import streamlit as st
from utils.openai_client import generate_article
from utils.exporters import to_markdown, to_docx_bytes, to_json_bytes
from utils.quality_checks import run_quality_report
from utils.meta_generator import generate_meta
from utils.internal_links import parse_inventory, suggest_internal_links
from utils.style_diversity import style_diversity_report
from utils.section_tools import list_sections, extract_section_text, regenerate_section
from utils.text_cleanup import clean_article
from utils.heading_tools import normalize_headings
from utils.enhanced_fix import run_enhanced_fix

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="ğŸ“ Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…",
    page_icon="ğŸŒ™",
    layout="wide"
)

st.title("ğŸ“ Ù†Ø¸Ø§Ù… ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…")
st.markdown(
    "Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©ØŒ ÙˆØ§Ø®ØªØ± Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„. "
    "ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙØ¹ÙŠÙ„ Ø¥Ù†Ø´Ø§Ø¡ Outline Ù‚Ø¨Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙˆÙ‚ÙÙ„ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù‡ Ø­Ø±ÙÙŠÙ‹Ø§."
)

# ===== Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… =====
keyword = st.text_input("ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ù…Ø«Ø§Ù„: ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø­Ø± ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù…)")

related_keywords_raw = st.text_area(
    "ğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© (Ø³Ø·Ø± Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©)",
    height=120,
    placeholder="ØªÙØ³ÙŠØ± Ø§Ù„ØºØ±Ù‚ ÙÙŠ Ø§Ù„Ø¨Ø­Ø±\nØªÙØ³ÙŠØ± Ø§Ù„Ø³Ø¨Ø§Ø­Ø©\nØ§Ù„Ù…ÙˆØ¬ Ø§Ù„Ø¹Ø§Ù„ÙŠ"
)

length_option_label = st.radio(
    "ğŸ“ Ø§Ø®ØªØ± Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„:",
    ("Ù‚ØµÙŠØ± (600-800 ÙƒÙ„Ù…Ø©)", "Ù…ØªÙˆØ³Ø· (900-1200 ÙƒÙ„Ù…Ø©)", "Ù…ÙˆØ³Ø¹ (1300-1600 ÙƒÙ„Ù…Ø©)")
)

tone = st.selectbox("ğŸ™ï¸ Ø§Ù„Ù†Ø¨Ø±Ø©", ["Ù‡Ø§Ø¯Ø¦Ø©", "Ù‚ØµØµÙŠØ©", "ØªØ­Ù„ÙŠÙ„ÙŠØ©"])

col_outline_a, col_outline_b = st.columns(2)
with col_outline_a:
    include_outline = st.toggle("ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Outline Ù‚Ø¨Ù„ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„", value=False)
with col_outline_b:
    enforce_outline = st.checkbox("ğŸ”’ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù€ Outline Ø­Ø±ÙÙŠÙ‹Ø§ (Ø¥Ù† ÙˆÙØ¬Ø¯)", value=True)

# Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
st.markdown("### ğŸ§­ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
inventory_raw = st.text_area(
    "Ø¶Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§Ù„Ø§Øª Ù…ÙˆÙ‚Ø¹Ùƒ Ø¨ØµÙŠØºØ© JSON (title, url, tags). Ù…Ø«Ø§Ù„:",
    height=160,
    value='''[
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø­Ø±","url":"/sea-dream","tags":["Ø§Ù„Ø¨Ø­Ø±","Ø§Ù„Ù…ÙˆØ¬","Ø§Ù„Ø³Ø¨Ø§Ø­Ø©"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ù…Ø§Ù„","url":"/money-dream","tags":["Ù…Ø§Ù„","Ø±Ø²Ù‚","Ø¯ÙŠÙˆÙ†"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„ØºØ±Ù‚","url":"/drowning-dream","tags":["Ø§Ù„ØºØ±Ù‚","Ø§Ù„Ø¨Ø­Ø±","Ø§Ù„Ø®ÙˆÙ"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø³Ø¨Ø§Ø­Ø©","url":"/swim-dream","tags":["Ø³Ø¨Ø§Ø­Ø©","Ù…Ø§Ø¡","Ø«Ù‚Ø©"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø°Ù‡Ø¨","url":"/gold-dream","tags":["Ø°Ù‡Ø¨","Ù…Ø§Ù„","Ø²ÙŠÙ†Ø©"]}
]'''
)

# ÙƒÙˆØ±Ø¨Ø³ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
st.markdown("### ğŸ§ª Ù…Ù‚Ø§Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
previous_corpus_raw = st.text_area(
    "Ø£Ù„ØµÙ‚ Ù…Ù‚Ø§Ù„Ø§ØªÙƒ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©. ØªÙ‚Ø¨Ù„ Ø§Ù„ØµÙŠØºØªÙŠÙ†:\n"
    "1) JSON: [{\"title\":\"...\",\"content\":\"...\"}, ...]\n"
    "2) Ù†Øµ Ù…ÙØµÙˆÙ„ Ø¨Ù€ --- Ø­ÙŠØ« Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ù…Ø­ØªÙˆÙ‰ Ù„ÙƒÙ„ Ù…Ù‚Ø§Ù„.",
    height=200,
    value=""
)

# âš™ï¸ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© (ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø¨Ø§Ø´Ø±Ø©)
st.markdown("### âš™ï¸ Ø£Ù‚Ø³Ø§Ù… Ù…Ø­Ø³Ù‘Ù†Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
col_opt1, col_opt2, col_opt3 = st.columns(3)
with col_opt1:
    enable_editor_note = st.checkbox("ØªØ¹Ù„ÙŠÙ‚ Ù…Ø­Ø±Ù‘Ø±", value=True)
    enable_not_applicable = st.checkbox("Ù…ØªÙ‰ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ Ø§Ù„ØªÙØ³ÙŠØ±ØŸ", value=True)
    enable_methodology = st.checkbox("Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙØ³ÙŠØ±", value=True)
with col_opt2:
    enable_sources = st.checkbox("Ù…ØµØ§Ø¯Ø± ØµØ±ÙŠØ­Ø©", value=True)
    enable_scenarios = st.checkbox("Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ©", value=True)
    scenarios_count = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª", 3, 5, 3)
with col_opt3:
    enable_faq = st.checkbox("Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø©", value=True)
    faq_count = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©", 3, 6, 4)
    enable_comparison = st.checkbox("Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚ÙŠÙ‚Ø©", value=True)

# ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø§Ù„Ø·ÙˆÙ„ Ø¥Ù„Ù‰ preset Ø¯Ø§Ø®Ù„ÙŠ
def _length_preset_from_label(label: str) -> str:
    if label.startswith("Ù‚ØµÙŠØ±"):
        return "short"
    if label.startswith("Ù…ÙˆØ³Ø¹"):
        return "long"
    return "medium"

length_preset = _length_preset_from_label(length_option_label)
related_keywords = [k.strip() for k in related_keywords_raw.splitlines() if k.strip()]

# Ø­Ø§ÙØ¸Ø© Ø§Ù„Ø­Ø§Ù„Ø©
if "result" not in st.session_state:
    st.session_state["result"] = None
if "keyword" not in st.session_state:
    st.session_state["keyword"] = ""
if "related_keywords" not in st.session_state:
    st.session_state["related_keywords"] = []
if "tone" not in st.session_state:
    st.session_state["tone"] = tone

# ===== ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ =====
if st.button("ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ù„"):
    if not keyword.strip():
        st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ù‹Ø§.")
        st.stop()

    with st.spinner("âœï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© GPTâ€¦"):
        try:
            result = generate_article(
                keyword=keyword.strip(),
                related_keywords=related_keywords,
                length_preset=length_preset,   # short/medium/long
                tone=tone,                     # Ù‡Ø§Ø¯Ø¦Ø©/Ù‚ØµØµÙŠØ©/ØªØ­Ù„ÙŠÙ„ÙŠØ©
                include_outline=include_outline,
                enable_editor_note=enable_editor_note,
                enable_not_applicable=enable_not_applicable,
                enable_methodology=enable_methodology,
                enable_sources=enable_sources,
                enable_scenarios=enable_scenarios,
                enable_faq=enable_faq,
                enable_comparison=enable_comparison,
                scenarios_count=scenarios_count,
                faq_count=faq_count,
                enforce_outline=enforce_outline,   # <<< Ø§Ù„Ø¬Ø¯ÙŠØ¯
            )
        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            st.stop()

    # ØªØ­Ø³ÙŠÙ†/ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù…ÙŠØªØ§ (Ø°ÙƒÙŠØŒ â‰¤155 Ø­Ø±Ù Ù„Ù„ÙˆØµÙ)
    try:
        improved_meta = generate_meta(keyword.strip(), result["article"])
        if improved_meta.get("title") or improved_meta.get("description"):
            result["meta"] = improved_meta
    except Exception:
        pass

    # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø©
    st.session_state["result"] = result
    st.session_state["keyword"] = keyword.strip()
    st.session_state["related_keywords"] = related_keywords
    st.session_state["tone"] = tone
    st.session_state["scenarios_count"] = scenarios_count
    st.session_state["faq_count"] = faq_count

# ===== Ø§Ù„Ø¹Ø±Ø¶ =====
if st.session_state["result"]:
    result = st.session_state["result"]  # Ù„Ù„Ø³Ù‡ÙˆÙ„Ø©
    col1, col2 = st.columns([2, 1], gap="large")

    with col1:
        if include_outline and result.get("outline"):
            st.subheader("ğŸ“ Outline Ø§Ù„Ù…Ù‚ØªØ±Ø­")
            st.markdown(result["outline"])

        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
        st.markdown(result["article"])  # ÙŠØ¹Ø±Ø¶ Markdown Ù…Ø¨Ø§Ø´Ø±Ø©

    with col2:
        st.subheader("ğŸ” Meta (SEO)")
        meta = result.get("meta", {})
        st.write(f"**Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Title):** {meta.get('title', '')}")
        st.write(f"**Ø§Ù„ÙˆØµÙ (Description):** {meta.get('description', '')}")

        # Ø²Ø± ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§ ÙŠØ¯ÙˆÙŠÙ‹Ø§
        if st.button("âœ¨ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§ (Ø¹Ù†ÙˆØ§Ù† + ÙˆØµÙ)"):
            try:
                improved_meta_btn = generate_meta(st.session_state["keyword"], result["article"])
                if improved_meta_btn:
                    meta = improved_meta_btn
                    result["meta"] = improved_meta_btn
                    st.session_state["result"] = result
                    st.success("ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§.")
                    st.write(f"**Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Title):** {meta.get('title', '')}")
                    st.write(f"**Ø§Ù„ÙˆØµÙ (Description):** {meta.get('description', '')}")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§: {e}")

        st.subheader("âœ… Quality Gates (Ù…Ø®ØªØµØ±)")
        qn = result.get("quality_notes", {})
        st.write(f"- **Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù:** {qn.get('length_target')}")
        st.write(f"- **Ø§Ù„Ø¨Ù†ÙŠØ©:** {qn.get('sections_target')}")
        st.write(f"- **Ø§Ù„Ù†Ø¨Ø±Ø©:** {qn.get('tone')}")
        st.write(f"- **Outline Ù…ÙØ³ØªØ®Ø¯Ù…ØŸ** {'Ù†Ø¹Ù…' if qn.get('outline_used') else 'Ù„Ø§'}")
        st.write("**Ù‚ÙˆØ§Ø¹Ø¯ Ù…ÙØ·Ø¨Ù‘Ù‚Ø©:**")
        st.write(", ".join(qn.get("enforced_rules", [])))

        # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
        st.subheader("ğŸ§ª ØªÙ‚Ø±ÙŠØ± Quality Gates (ØªÙØµÙŠÙ„ÙŠ)")
        rep = run_quality_report(result["article"], expected_length_preset=length_preset)

        # Ø¹Ø±Ø¶ Ù†Ù‚Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ©
        st.write(f"**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±:** {rep.get('risk_level')}")
        m = rep.get("metrics", {})
        st.write(
            f"- ÙƒÙ„Ù…Ø§Øª: {m.get('words')} | H2: {m.get('h2_count')} | H3: {m.get('h3_count')}\n"
            f"- Ø¹Ø¨Ø§Ø±Ø§Øª Ø¬Ø²Ù…: {m.get('certainty_hits')} | Ø­Ø´Ùˆ: {m.get('filler_hits')} | Ù…ÙØ±Ø¯Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {m.get('probability_lexicon_hits')}\n"
            f"- ØªÙ†ÙˆÙŠÙ‡ Ù…ÙˆØ¬ÙˆØ¯ØŸ {'Ù†Ø¹Ù…' if m.get('disclaimer_present') else 'Ù„Ø§'} | Ø£Ù‚Ø³Ø§Ù… Ù†Ø§Ù‚ØµØ©: {', '.join(m.get('missing_sections', [])) or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}\n"
            f"- Ø¥Ø´Ø§Ø±Ø§Øª PAA: {m.get('paa_signals')}"
        )

        # ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ©
        actions = rep.get("suggested_actions", [])
        if actions:
            st.write("**ØªÙˆØµÙŠØ§Øª Ø¥ØµÙ„Ø§Ø­:**")
            for a in actions:
                st.write(f"â€¢ {a}")
        else:
            st.write("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØµÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ© â€” Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…ØªÙˆØ§Ø²Ù† ğŸ‘")

        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
        st.subheader("ğŸ“¤ ØªØµØ¯ÙŠØ±")
        md_bytes = to_markdown(result["article"]).encode("utf-8")
        docx_bytes = to_docx_bytes(result["article"], meta_title=meta.get("title", ""))
        json_bytes = to_json_bytes(
            article_markdown=result["article"],
            meta=meta,
            keyword=st.session_state["keyword"],
            related_keywords=st.session_state["related_keywords"],
        )

        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Markdown",
            data=md_bytes,
            file_name="article.md",
            mime="text/markdown"
        )
        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ DOCX",
            data=docx_bytes,
            file_name="article.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ JSON",
            data=json_bytes,
            file_name="article.json",
            mime="application/json"
        )

    # ===== Ø§Ù„ÙÙ„ØªØ±Ø©/Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© =====
    st.markdown("---")
    st.subheader("ğŸ§¹ ÙÙ„ØªØ±Ø©/ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„ØºÙˆÙŠØ©")

    col_clean_a, col_clean_b = st.columns(2)
    with col_clean_a:
        opt_fix_punct = st.checkbox("ØªØµØ­ÙŠØ­ Ù…Ø³Ø§ÙØ§Øª/Ø´ÙƒÙ„ Ø§Ù„ØªØ±Ù‚ÙŠÙ…", value=True)
        opt_remove_filler = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø¬ÙÙ…Ù„/Ø£Ø³Ø·Ø± Ø­Ø´ÙˆÙŠØ©", value=True)
    with col_clean_b:
        opt_normalize_ws = st.checkbox("ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø±", value=True)
        opt_aggressive = st.checkbox("Ù†Ù…Ø· Ø¹Ø¯ÙˆØ§Ù†ÙŠ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø´Ùˆ (Ø­Ø°Ø±)", value=False)

    if st.button("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ù„ØºÙˆÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚"):
        with st.spinner("ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„..."):
            cleaned = clean_article(
                result["article"],
                remove_filler=opt_remove_filler,
                aggressive=opt_aggressive,
                fix_punct=opt_fix_punct,
                normalize_ws=opt_normalize_ws,
            )
            result["article"] = cleaned["cleaned"]
            st.session_state["result"] = result
            repc = cleaned["report"]
            st.success("ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")
            st.write("**ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ:**")
            st.write(
                f"- ØªØµØ­ÙŠØ­ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø°Ù: {repc['ellipsis_fixed']} | ØªÙ‚Ù„ÙŠØµ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {repc['repeated_punct_collapsed']}\n"
                f"- ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø³ØªØ¨Ø¯Ù„Ø©: {repc['arabic_comma_applied']} | Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§ÙØ§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {repc['spacing_fixed']}\n"
                f"- ØªØ·Ø¨ÙŠØ¹ Ù…Ø³Ø§ÙØ§Øª/Ø£Ø³Ø·Ø±: {repc['whitespace_normalized']} | Ø£Ø³Ø·Ø±/Ø¬Ù…Ù„ Ø­ÙØ°ÙØª/Ù‚ÙØµÙ‘Øª: {repc['filler_removed']}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:**")
            st.markdown(result["article"])

    # ===== Normalize Headings =====
    st.markdown("---")
    st.subheader("ğŸ§­ Normalize Headings (ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†)")

    col_h_a, col_h_b = st.columns(2)
    with col_h_a:
        opt_h1_to_h2 = st.checkbox("ØªØ­ÙˆÙŠÙ„ H1 Ø¥Ù„Ù‰ H2", value=True)
        opt_h4_to_h3 = st.checkbox("Ø¥Ù†Ø²Ø§Ù„ H4+ Ø¥Ù„Ù‰ H3", value=True)
        opt_space_after_hash = st.checkbox("ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨Ø¹Ø¯ #", value=True)
    with col_h_b:
        opt_trim_punct = st.checkbox("Ø¥Ø²Ø§Ù„Ø© ØªØ±Ù‚ÙŠÙ… Ø²Ø§Ø¦Ø¯ Ø¨Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", value=True)
        opt_collapse_spaces = st.checkbox("Ø¶ØºØ· Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", value=True)
        opt_dedupe = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…ØªØªØ§Ù„ÙŠØ© Ù…ÙƒØ±Ø±Ø©", value=True)

    opt_autonumber = st.checkbox("ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø£Ù‚Ø³Ø§Ù… (H2/H3)", value=False)

    if st.button("ğŸ§­ Ø·Ø¨Ù‘Ù‚ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†"):
        with st.spinner("ØªØ·Ø¨ÙŠÙ‚ Normalize Headings..."):
            nh = normalize_headings(
                result["article"],
                h1_to_h2=opt_h1_to_h2,
                h4plus_to_h3=opt_h4_to_h3,
                unify_space_after_hash=opt_space_after_hash,
                trim_trailing_punct=opt_trim_punct,
                collapse_spaces=opt_collapse_spaces,
                remove_consecutive_duplicates=opt_dedupe,
                autonumber=opt_autonumber,
            )
            result["article"] = nh["normalized"]
            st.session_state["result"] = result

            ch = nh["changes"]
            st.success("ØªÙ… ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†.")
            st.write("**Ù…Ù„Ø®Øµ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª:**")
            st.write(
                f"- H1â†’H2: {ch['h1_to_h2']} | H4+â†’H3: {ch['h4plus_to_h3']} | Ù…Ø³Ø§ÙØ© Ø¨Ø¹Ø¯ #: {ch['space_after_hash_fixed']}\n"
                f"- Ø¥Ø²Ø§Ù„Ø© ØªØ±Ù‚ÙŠÙ… Ù†Ù‡Ø§Ø¦ÙŠ: {ch['trimmed_trailing_punct']} | Ø¶ØºØ· Ù…Ø³Ø§ÙØ§Øª: {ch['collapsed_spaces']} | Ø­Ø°Ù Ù…ÙƒØ±Ø±: {ch['deduped_headings']}\n"
                f"- ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ: {ch['autonumbered']} | Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†: {ch['total_headings']}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹:**")
            st.markdown(result["article"])

    # ===== Ø¯ÙØ¹Ø© Ø¥ØµÙ„Ø§Ø­ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©) =====
    st.markdown("---")
    st.subheader("ğŸš‘ Ø¯ÙØ¹Ø© Ø¥ØµÙ„Ø§Ø­ (ØªÙ†Ø¸ÙŠÙ + Normalize + Ø®Ø§ØªÙ…Ø© Ù…Ø³Ø¤ÙˆÙ„Ø©)")

    col_fix_a, col_fix_b = st.columns(2)
    with col_fix_a:
        fx_fix_punct = st.checkbox("ØªØµØ­ÙŠØ­ Ø§Ù„ØªØ±Ù‚ÙŠÙ…/Ø§Ù„Ù…Ø³Ø§ÙØ§Øª", value=True, key="fx_fix_punct")
        fx_remove_filler = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„ÙˆØ§Ø¶Ø­", value=True, key="fx_remove_filler")
        fx_normalize_ws = st.checkbox("ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø±", value=True, key="fx_normalize_ws")
    with col_fix_b:
        fx_h1_to_h2 = st.checkbox("H1â†’H2", value=True, key="fx_h1_to_h2")
        fx_h4_to_h3 = st.checkbox("H4+â†’H3", value=True, key="fx_h4_to_h3")
        fx_autonumber = st.checkbox("ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ H2/H3", value=False, key="fx_autonumber")

    if st.button("ğŸš‘ Ø·Ø¨Ù‘Ù‚ Ø¯ÙØ¹Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¢Ù†"):
        if not result or not result.get("article"):
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„ Ù…ÙÙˆÙ„Ø¯ Ø¨Ø¹Ø¯.")
        else:
            with st.spinner("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙØ¹Ø©â€¦"):
                # 1) ØªÙ†Ø¸ÙŠÙ
                cleaned = clean_article(
                    result["article"],
                    remove_filler=fx_remove_filler,
                    aggressive=False,
                    fix_punct=fx_fix_punct,
                    normalize_ws=fx_normalize_ws,
                )
                article_fx = cleaned["cleaned"]
                rep_clean = cleaned["report"]

                # 2) Normalize Headings
                nh = normalize_headings(
                    article_fx,
                    h1_to_h2=fx_h1_to_h2,
                    h4plus_to_h3=fx_h4_to_h3,
                    unify_space_after_hash=True,
                    trim_trailing_punct=True,
                    collapse_spaces=True,
                    remove_consecutive_duplicates=True,
                    autonumber=fx_autonumber,
                )
                article_fx = nh["normalized"]
                rep_head = nh["changes"]

                # 3) Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø®Ø§ØªÙ…Ø©
                secs = list_sections(article_fx)
                outro_title = None
                for t, lvl, s, e in secs:
                    if re.fullmatch(r"Ø®Ø§ØªÙ…Ø©", t):
                        outro_title = t
                        break

                if outro_title:
                    try:
                        article_fx = regenerate_section(
                            article_md=article_fx,
                            section_title=outro_title,
                            keyword=st.session_state.get("keyword", ""),
                            related_keywords=st.session_state.get("related_keywords", []),
                            tone=st.session_state.get("tone", "Ù‡Ø§Ø¯Ø¦Ø©"),
                            section_type="outro",
                        )
                        fx_outro_done = True
                    except Exception as e:
                        fx_outro_done = False
                        st.warning(f"ØªØ¹Ø°Ù‘Ø±Øª Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø®Ø§ØªÙ…Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§: {e}")
                else:
                    fx_outro_done = False
                    st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… Â«Ø®Ø§ØªÙ…Ø©Â». ØªØ®Ø·Ù‘ÙŠÙ†Ø§ Ø®Ø·ÙˆØ© Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø®Ø§ØªÙ…Ø©.")

                result["article"] = article_fx
                st.session_state["result"] = result

            st.success("âœ… ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙØ¹Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­.")
            st.write("**Ù…Ù„Ø®Øµ Ø§Ù„Ø¯ÙØ¹Ø©:**")
            st.write(
                f"- ØªÙ†Ø¸ÙŠÙ: Ø­Ø°Ù/Ù‚Øµ Ø§Ù„Ø­Ø´Ùˆ = {rep_clean['filler_removed']} | ØªÙ‚Ù„ÙŠØµ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ±Ù‚ÙŠÙ… = {rep_clean['repeated_punct_collapsed']} | ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ© = {rep_clean['arabic_comma_applied']}\n"
                f"- Normalize: H1â†’H2 = {rep_head['h1_to_h2']} | H4+â†’H3 = {rep_head['h4plus_to_h3']} | Ø­Ø°Ù Ù…ÙƒØ±Ø± = {rep_head['deduped_headings']} | ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ = {rep_head['autonumbered']}\n"
                f"- Ø®Ø§ØªÙ…Ø© Ù…Ø³Ø¤ÙˆÙ„Ø©: {'ØªÙ…Øª' if fx_outro_done else 'ØªØ®Ø·Ù‘ÙŠÙ†Ø§/ØªØ¹Ø°Ù‘Ø±'}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„Ø¯ÙØ¹Ø©:**")
            st.markdown(result["article"])

    # ===== Ø¯ÙØ¹Ø© Ø¥ØµÙ„Ø§Ø­ Ù…ÙØ­Ø³Ù‘Ù†Ø© =====
    st.markdown("---")
    st.subheader("ğŸ› ï¸ Ø¯ÙØ¹Ø© Ø¥ØµÙ„Ø§Ø­ Ù…ÙØ­Ø³Ù‘Ù†Ø© (ØªÙ†Ø¸ÙŠÙ + Normalize + ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø²Ù… + Ø¥Ø¶Ø§ÙØ© 'Ù…ØªÙ‰ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚' + Ø®Ø§ØªÙ…Ø© Ù…Ø³Ø¤ÙˆÙ„Ø©)")

    col_efx_a, col_efx_b = st.columns(2)
    with col_efx_a:
        efx_fix_punct = st.checkbox("ØªØµØ­ÙŠØ­ Ø§Ù„ØªØ±Ù‚ÙŠÙ…/Ø§Ù„Ù…Ø³Ø§ÙØ§Øª", value=True, key="efx_fix_punct")
        efx_remove_filler = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„ÙˆØ§Ø¶Ø­", value=True, key="efx_remove_filler")
        efx_normalize_ws = st.checkbox("ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø±", value=True, key="efx_normalize_ws")
    with col_efx_b:
        efx_h1_to_h2 = st.checkbox("H1â†’H2", value=True, key="efx_h1_to_h2")
        efx_h4_to_h3 = st.checkbox("H4+â†’H3", value=True, key="efx_h4_to_h3")
        efx_autonumber = st.checkbox("ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ H2/H3", value=False, key="efx_autonumber")

    if st.button("ğŸ› ï¸ Ø·Ø¨Ù‘Ù‚ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø© Ø§Ù„Ø¢Ù†"):
        if not result or not result.get("article"):
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„ Ù…ÙÙˆÙ„Ø¯ Ø¨Ø¹Ø¯.")
        else:
            with st.spinner("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©â€¦"):
                fx = run_enhanced_fix(
                    result["article"],
                    keyword=st.session_state.get("keyword", ""),
                    related_keywords=st.session_state.get("related_keywords", []),
                    tone=st.session_state.get("tone", "Ù‡Ø§Ø¯Ø¦Ø©"),
                    clean_opts={
                        "remove_filler": efx_remove_filler,
                        "aggressive": False,
                        "fix_punct": efx_fix_punct,
                        "normalize_ws": efx_normalize_ws,
                    },
                    heading_opts={
                        "h1_to_h2": efx_h1_to_h2,
                        "h4plus_to_h3": efx_h4_to_h3,
                        "unify_space_after_hash": True,
                        "trim_trailing_punct": True,
                        "collapse_spaces": True,
                        "remove_consecutive_duplicates": True,
                        "autonumber": efx_autonumber,
                    }
                )
                result["article"] = fx["article"]
                st.session_state["result"] = result

            st.success("âœ… ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©.")
            rep2 = fx["reports"]
            st.write("**Ù…Ù„Ø®Øµ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©:**")
            st.write(
                f"- ØªÙ†Ø¸ÙŠÙ: Ø­Ø°Ù/Ù‚Øµ Ø§Ù„Ø­Ø´Ùˆ = {rep2['clean']['filler_removed']} | ØªÙƒØ±Ø§Ø± ØªØ±Ù‚ÙŠÙ… = {rep2['clean']['repeated_punct_collapsed']} | ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ© = {rep2['clean']['arabic_comma_applied']}\n"
                f"- Normalize: H1â†’H2 = {rep2['headings']['h1_to_h2']} | H4+â†’H3 = {rep2['headings']['h4plus_to_h3']} | Ø­Ø°Ù Ù…ÙƒØ±Ø± = {rep2['headings']['deduped_headings']} | ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ = {rep2['headings']['autonumbered']}\n"
                f"- ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø²Ù…: Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª = {rep2['soften']['replacements_count']}\n"
                f"- Ø¥Ø¯Ø±Ø§Ø¬ 'Ù…ØªÙ‰ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ØŸ': {'ØªÙ…' if rep2['not_applicable_inserted'] else 'ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§'}\n"
                f"- Ø®Ø§ØªÙ…Ø© Ù…Ø³Ø¤ÙˆÙ„Ø©: {'ØªÙ…Øª' if rep2['outro_regenerated'] else 'ØªØ®Ø·Ù‘ÙŠÙ†Ø§/ØªØ¹Ø°Ù‘Ø±'}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©:**")
            st.markdown(result["article"])

    # ===== Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© =====
    inventory = parse_inventory(inventory_raw)
    if inventory:
        st.subheader("ğŸ”— Ø§Ù‚ØªØ±Ø§Ø­ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©")
        suggestions = suggest_internal_links(
            keyword=st.session_state["keyword"],
            related_keywords=st.session_state["related_keywords"],
            article_markdown=result["article"],
            inventory=inventory,
            top_k=6,
        )
        if suggestions:
            for s in suggestions:
                st.markdown(f"- [{s['title']}]({s['url']}) â€” **Score:** {s['score']}")
        else:
            st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§Øª ÙƒØ§ÙÙŠØ© Ù…Ø¹ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ.")
    else:
        st.caption("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø®Ø²ÙˆÙ† Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©Ø› Ø£Ø¶ÙÙ JSON ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ Ø£Ø¹Ù„Ø§Ù‡ Ù„Ø±Ø¤ÙŠØ© Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª.")

    # ===== Ù…Ø¤Ø´Ù‘Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ =====
    if previous_corpus_raw and previous_corpus_raw.strip():
        st.subheader("ğŸ­ Ù…Ø¤Ø´Ù‘Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨")
        srep = style_diversity_report(result["article"], previous_corpus_raw, top_k=5)

        st.write(f"**Ø­Ø¬Ù… ÙƒÙˆØ±Ø¨Ø³ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:** {srep.get('corpus_size')}")
        st.write(f"**Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Jaccard 3-grams):** {srep.get('avg_similarity')}")
        st.write(f"**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±:** {srep.get('risk_level')}")

        sm = srep.get("style_metrics", {})
        st.write(
            f"- TTR (ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª): {sm.get('type_token_ratio')}\n"
            f"- Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø© (ØªÙˆÙƒÙ†Ø²): {sm.get('avg_sentence_length_tokens')}\n"
            f"- ÙƒØ«Ø§ÙØ© Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {sm.get('punctuation_density')} | Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„: {sm.get('sentences_count')} | Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²: {sm.get('tokens_count')}"
        )

        top_sim = srep.get("top_similar", [])
        if top_sim:
            st.write("**Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªØ´Ø§Ø¨Ù‡Ù‹Ø§:**")
            for i, item in enumerate(top_sim, 1):
                st.write(f"{i}. {item['title']} â€” similarity: {item['similarity']}")
        else:
            st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨Ø¯Ø±Ø¬Ø© Ù…Ù„Ø­ÙˆØ¸Ø©.")

        sugg = srep.get("suggestions", [])
        if sugg:
            st.write("**ØªÙˆØµÙŠØ§Øª ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡:**")
            for s in sugg:
                st.write(f"â€¢ {s}")
        else:
            st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØµÙŠØ§Øª â€” Ø§Ù„ØªÙ†ÙˆØ¹ Ø¬ÙŠØ¯ ğŸ‘")
    else:
        st.caption("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ù‚Ø§Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ©.")

    # ===== ØªØ­Ø±ÙŠØ± Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯ (Ø£Ø²Ø±Ø§Ø± Ø³Ø±ÙŠØ¹Ø© + ØªØ­Ø¯ÙŠØ¯ ÙŠØ¯ÙˆÙŠ) =====
    st.markdown("---")
    st.subheader("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯")

    secs = list_sections(result["article"])
    if not secs:
        st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ÙˆÙŠÙ† H2/H3 Ù„Ø§Ø®ØªÙŠØ§Ø± Ù‚Ø³Ù….")
    else:
        titles = [t for (t, lvl, s, e) in secs]

        # Ø£Ø²Ø±Ø§Ø± Ø³Ø±ÙŠØ¹Ø©
        quick_defs = [
            ("Ø§ÙØªØªØ§Ø­ÙŠØ©", "intro",  None, [r"Ø§ÙØªØªØ§Ø­ÙŠØ©"]),
            ("Ù„Ù…Ø§Ø°Ø§ Ù‚Ø¯ ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ù…Ø²ØŸ", "why", None, [r"Ù„Ù…Ø§Ø°Ø§\s+Ù‚Ø¯\s+ÙŠØ¸Ù‡Ø±\s+Ø§Ù„Ø±Ù…Ø²\ØŸ?"]),
            ("Ù…ØªÙ‰ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ Ø§Ù„ØªÙØ³ÙŠØ±ØŸ", "not_applicable", None, [r"Ù…ØªÙ‰\s+Ù„Ø§\s+ÙŠÙ†Ø·Ø¨Ù‚\s+Ø§Ù„ØªÙØ³ÙŠØ±\ØŸ?"]),
            ("Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ©", "scenarios", st.session_state.get("scenarios_count", 3), [r"Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª\s+ÙˆØ§Ù‚Ø¹ÙŠØ©"]),
            ("Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø©", "faq", st.session_state.get("faq_count", 4), [r"Ø£Ø³Ø¦Ù„Ø©\s+Ø´Ø§Ø¦Ø¹Ø©"]),
            ("Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚ÙŠÙ‚Ø©", "comparison", None, [r"Ù…Ù‚Ø§Ø±Ù†Ø©\s+Ø¯Ù‚ÙŠÙ‚Ø©"]),
            ("Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙØ³ÙŠØ±", "methodology", None, [r"Ù…Ù†Ù‡Ø¬ÙŠØ©\s+Ø§Ù„ØªÙØ³ÙŠØ±"]),
            ("Ø®Ø§ØªÙ…Ø©", "outro", None, [r"Ø®Ø§ØªÙ…Ø©", r"Ø§Ù„Ø®Ù„Ø§ØµØ©"]),
        ]

        st.write("**Ø£Ø²Ø±Ø§Ø± Ø³Ø±ÙŠØ¹Ø©:**")
        cols = st.columns(4)
        for i, (label, sec_type, target_count, patterns) in enumerate(quick_defs):
            col = cols[i % 4]
            with col:
                if st.button(label):
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚
                    picked_title = None
                    for t in titles:
                        for pat in patterns:
                            if re.fullmatch(pat, t):
                                picked_title = t
                                break
                        if picked_title:
                            break
                    if not picked_title:
                        st.warning(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… Ø¨Ø¹Ù†ÙˆØ§Ù† Â«{label}Â».")
                    else:
                        with st.spinner(f"Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ù‚Ø³Ù…: {picked_title} â€¦"):
                            try:
                                new_article = regenerate_section(
                                    article_md=result["article"],
                                    section_title=picked_title,
                                    keyword=st.session_state.get("keyword", ""),
                                    related_keywords=st.session_state.get("related_keywords", []),
                                    tone=st.session_state.get("tone", "Ù‡Ø§Ø¯Ø¦Ø©"),
                                    section_type=sec_type,
                                    # ØªÙ… ØªÙ…Ø±ÙŠØ± target_count Ù„ÙÙ€faq/scenarios ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© (Ø¥Ù† ÙƒØ§Ù†Øª ØªØ¯Ø¹Ù…Ù‡)
                                )
                                result["article"] = new_article
                                st.session_state["result"] = result
                                st.success(f"ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø³Ù… Â«{picked_title}Â».")
                            except Exception as e:
                                st.error(f"ØªØ¹Ø°Ù‘Ø±Øª Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù…: {e}")

        st.markdown("â€” Ø£Ùˆ â€”")

        # Ø§Ø®ØªÙŠØ§Ø± ÙŠØ¯ÙˆÙŠ
        manual_title = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù†ÙˆØ§Ù† Ù‚Ø³Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯Ù‡:", titles)
        manual_type = st.selectbox(
            "Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø³Ù… (ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù†Ø³Ø¨):",
            ["intro", "why", "not_applicable", "scenarios", "faq", "comparison", "methodology", "outro"],
            index=0
        )
        manual_extra = ""
        if manual_type in ("scenarios", "faq"):
            manual_extra = st.text_input("Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ): Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± (Ù…Ø«Ø§Ù„: 3)")

        if st.button("ğŸ” Ø£Ø¹Ø¯ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯"):
            with st.spinner("ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù…..."):
                try:
                    # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¹Ø¯Ø¯ Ø¥Ù† Ø£ÙØ¯Ø®Ù„
                    extra_kwargs = {}
                    if manual_type in ("scenarios", "faq"):
                        try:
                            n = int(manual_extra.strip()) if manual_extra.strip() else None
                            if n:
                                extra_kwargs["target_count"] = n
                        except Exception:
                            pass

                    updated = regenerate_section(
                        article_md=result["article"],
                        section_title=manual_title,
                        keyword=st.session_state.get("keyword", ""),
                        related_keywords=st.session_state.get("related_keywords", []),
                        tone=st.session_state.get("tone", "Ù‡Ø§Ø¯Ø¦Ø©"),
                        section_type=manual_type,
                        **extra_kwargs
                    )
                    result["article"] = updated
                    st.session_state["result"] = result
                    st.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø³Ù… Ø¨Ù†Ø¬Ø§Ø­.")
                    st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„:**")
                    st.markdown(result["article"])
                except Exception as e:
                    st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
