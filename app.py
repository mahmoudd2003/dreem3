# app.py
# ===========================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit Ù„Ù†Ø¸Ø§Ù… ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…
# ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰:
# - utils/openai_client.py    (generate_article)
# - utils/exporters.py        (ØªØµØ¯ÙŠØ± Markdown/DOCX/JSON)
# - utils/quality_checks.py   (ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠ)
# - utils/meta_generator.py   (Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ÙŠØªØ§ Ø§Ù„Ø°ÙƒÙŠ)
# - utils/internal_links.py   (Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)
# - utils/style_diversity.py  (Ù…Ø¤Ø´Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨)
# - utils/section_tools.py    (ØªØ­Ø±ÙŠØ± Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯)
# - utils/text_cleanup.py     (ÙÙ„ØªØ±Ø©/ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„ØºÙˆÙŠØ©)
# - utils/heading_tools.py    (Normalize Headings)
# ===========================

import streamlit as st
from utils.openai_client import generate_article
from utils.exporters import to_markdown, to_docx_bytes, to_json_bytes
from utils.quality_checks import run_quality_report
from utils.meta_generator import generate_meta
from utils.internal_links import parse_inventory, suggest_internal_links
from utils.style_diversity import style_diversity_report
from utils.section_tools import list_sections, extract_section_text, regenerate_section
from utils.text_cleanup import clean_article
from utils.heading_tools import normalize_headings

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="ğŸ“ Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…",
    page_icon="ğŸŒ™",
    layout="wide"
)

st.title("ğŸ“ Ù†Ø¸Ø§Ù… ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…")
st.markdown(
    "Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©ØŒ ÙˆØ§Ø®ØªØ± Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„. "
    "ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙØ¹ÙŠÙ„ Ø¥Ù†Ø´Ø§Ø¡ Outline Ù‚Ø¨Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø©."
)

# ===== Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… =====
keyword = st.text_input("ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ù…Ø«Ø§Ù„: ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø­Ø± ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù…)")

related_keywords_raw = st.text_area(
    "ğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© (Ø³Ø·Ø± Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©)",
    height=120,
    placeholder="ØªÙØ³ÙŠØ± Ø§Ù„ØºØ±Ù‚ ÙÙŠ Ø§Ù„Ø¨Ø­Ø±\nØªÙØ³ÙŠØ± Ø§Ù„Ø³Ø¨Ø§Ø­Ø©\nØ§Ù„Ù…ÙˆØ¬ Ø§Ù„Ø¹Ø§Ù„ÙŠ"
)

length_option_label = st.radio(
    "ğŸ“ Ø§Ø®ØªØ± Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„:",
    ("Ù‚ØµÙŠØ± (600-800 ÙƒÙ„Ù…Ø©)", "Ù…ØªÙˆØ³Ø· (900-1200 ÙƒÙ„Ù…Ø©)", "Ù…ÙˆØ³Ø¹ (1300-1600 ÙƒÙ„Ù…Ø©)")
)

tone = st.selectbox("ğŸ™ï¸ Ø§Ù„Ù†Ø¨Ø±Ø©", ["Ù‡Ø§Ø¯Ø¦Ø©", "Ù‚ØµØµÙŠØ©", "ØªØ­Ù„ÙŠÙ„ÙŠØ©"])

include_outline = st.toggle("ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Outline Ù‚Ø¨Ù„ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„", value=False)

# Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
st.markdown("### ğŸ§­ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
inventory_raw = st.text_area(
    "Ø¶Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§Ù„Ø§Øª Ù…ÙˆÙ‚Ø¹Ùƒ Ø¨ØµÙŠØºØ© JSON (title, url, tags). Ù…Ø«Ø§Ù„:",
    height=160,
    value='''[
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø­Ø±","url":"/sea-dream","tags":["Ø§Ù„Ø¨Ø­Ø±","Ø§Ù„Ù…ÙˆØ¬","Ø§Ù„Ø³Ø¨Ø§Ø­Ø©"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ù…Ø§Ù„","url":"/money-dream","tags":["Ù…Ø§Ù„","Ø±Ø²Ù‚","Ø¯ÙŠÙˆÙ†"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„ØºØ±Ù‚","url":"/drowning-dream","tags":["Ø§Ù„ØºØ±Ù‚","Ø§Ù„Ø¨Ø­Ø±","Ø§Ù„Ø®ÙˆÙ"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø³Ø¨Ø§Ø­Ø©","url":"/swim-dream","tags":["Ø³Ø¨Ø§Ø­Ø©","Ù…Ø§Ø¡","Ø«Ù‚Ø©"]},
  {"title":"ØªÙØ³ÙŠØ± Ø±Ø¤ÙŠØ© Ø§Ù„Ø°Ù‡Ø¨","url":"/gold-dream","tags":["Ø°Ù‡Ø¨","Ù…Ø§Ù„","Ø²ÙŠÙ†Ø©"]}
]'''
)

# ÙƒÙˆØ±Ø¨Ø³ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
st.markdown("### ğŸ§ª Ù…Ù‚Ø§Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
previous_corpus_raw = st.text_area(
    "Ø£Ù„ØµÙ‚ Ù…Ù‚Ø§Ù„Ø§ØªÙƒ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©. ØªÙ‚Ø¨Ù„ Ø§Ù„ØµÙŠØºØªÙŠÙ†:\n"
    "1) JSON: [{\"title\":\"...\",\"content\":\"...\"}, ...]\n"
    "2) Ù†Øµ Ù…ÙØµÙˆÙ„ Ø¨Ù€ --- Ø­ÙŠØ« Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ù…Ø­ØªÙˆÙ‰ Ù„ÙƒÙ„ Ù…Ù‚Ø§Ù„.",
    height=200,
    value=""
)

# ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø§Ù„Ø·ÙˆÙ„ Ø¥Ù„Ù‰ preset Ø¯Ø§Ø®Ù„ÙŠ
def _length_preset_from_label(label: str) -> str:
    if label.startswith("Ù‚ØµÙŠØ±"):
        return "short"
    if label.startswith("Ù…ÙˆØ³Ø¹"):
        return "long"
    return "medium"

length_preset = _length_preset_from_label(length_option_label)
related_keywords = [k.strip() for k in related_keywords_raw.splitlines() if k.strip()]

# Ø­Ø§ÙØ¸Ø© Ø§Ù„Ø­Ø§Ù„Ø©
if "result" not in st.session_state:
    st.session_state["result"] = None
if "keyword" not in st.session_state:
    st.session_state["keyword"] = ""
if "related_keywords" not in st.session_state:
    st.session_state["related_keywords"] = []
if "tone" not in st.session_state:
    st.session_state["tone"] = tone

# ===== ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ =====
if st.button("ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ù„"):
    if not keyword.strip():
        st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹.")
        st.stop()

    with st.spinner("âœï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© GPTâ€¦"):
        try:
            result = generate_article(
                keyword=keyword.strip(),
                related_keywords=related_keywords,
                length_preset=length_preset,   # short/medium/long
                tone=tone,                     # Ù‡Ø§Ø¯Ø¦Ø©/Ù‚ØµØµÙŠØ©/ØªØ­Ù„ÙŠÙ„ÙŠØ©
                include_outline=include_outline
            )
        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            st.stop()

    # ØªØ­Ø³ÙŠÙ†/ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù…ÙŠØªØ§ (Ø°ÙƒÙŠØŒ â‰¤155 Ø­Ø±Ù Ù„Ù„ÙˆØµÙ)
    try:
        improved_meta = generate_meta(keyword.strip(), result["article"])
        if improved_meta.get("title") or improved_meta.get("description"):
            result["meta"] = improved_meta
    except Exception:
        pass

    # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø©
    st.session_state["result"] = result
    st.session_state["keyword"] = keyword.strip()
    st.session_state["related_keywords"] = related_keywords
    st.session_state["tone"] = tone

# ===== Ø§Ù„Ø¹Ø±Ø¶ =====
if st.session_state["result"]:
    result = st.session_state["result"]  # Ù„Ù„Ø³Ù‡ÙˆÙ„Ø©
    col1, col2 = st.columns([2, 1], gap="large")

    with col1:
        if include_outline and result.get("outline"):
            st.subheader("ğŸ“ Outline Ø§Ù„Ù…Ù‚ØªØ±Ø­")
            st.markdown(result["outline"])

        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
        st.markdown(result["article"])  # ÙŠØ¹Ø±Ø¶ Markdown Ù…Ø¨Ø§Ø´Ø±Ø©

    with col2:
        st.subheader("ğŸ” Meta (SEO)")
        meta = result.get("meta", {})
        st.write(f"**Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Title):** {meta.get('title', '')}")
        st.write(f"**Ø§Ù„ÙˆØµÙ (Description):** {meta.get('description', '')}")

        # Ø²Ø± ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§ ÙŠØ¯ÙˆÙŠÙ‹Ø§
        if st.button("âœ¨ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§ (Ø¹Ù†ÙˆØ§Ù† + ÙˆØµÙ)"):
            try:
                improved_meta_btn = generate_meta(st.session_state["keyword"], result["article"])
                if improved_meta_btn:
                    meta = improved_meta_btn
                    result["meta"] = improved_meta_btn
                    st.session_state["result"] = result
                    st.success("ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§.")
                    st.write(f"**Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Title):** {meta.get('title', '')}")
                    st.write(f"**Ø§Ù„ÙˆØµÙ (Description):** {meta.get('description', '')}")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙŠØªØ§: {e}")

        st.subheader("âœ… Quality Gates (Ù…Ø®ØªØµØ±)")
        qn = result.get("quality_notes", {})
        st.write(f"- **Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù:** {qn.get('length_target')}")
        st.write(f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù:** {qn.get('sections_target')}")
        st.write(f"- **Ø§Ù„Ù†Ø¨Ø±Ø©:** {qn.get('tone')}")
        st.write(f"- **Outline Ù…ÙØ³ØªØ®Ø¯Ù…ØŸ** {'Ù†Ø¹Ù…' if qn.get('outline_used') else 'Ù„Ø§'}")
        st.write("**Ù‚ÙˆØ§Ø¹Ø¯ Ù…ÙØ·Ø¨Ù‘Ù‚Ø©:**")
        st.write(", ".join(qn.get("enforced_rules", [])))

        # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
        st.subheader("ğŸ§ª ØªÙ‚Ø±ÙŠØ± Quality Gates (ØªÙØµÙŠÙ„ÙŠ)")
        rep = run_quality_report(result["article"], expected_length_preset=length_preset)

        # Ø¹Ø±Ø¶ Ù†Ù‚Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ©
        st.write(f"**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±:** {rep.get('risk_level')}")
        m = rep.get("metrics", {})
        st.write(
            f"- ÙƒÙ„Ù…Ø§Øª: {m.get('words')} | H2: {m.get('h2_count')} | H3: {m.get('h3_count')}\n"
            f"- Ø¹Ø¨Ø§Ø±Ø§Øª Ø¬Ø²Ù…: {m.get('certainty_hits')} | Ø­Ø´Ùˆ: {m.get('filler_hits')} | Ù…ÙØ±Ø¯Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {m.get('probability_lexicon_hits')}\n"
            f"- ØªÙ†ÙˆÙŠÙ‡ Ù…ÙˆØ¬ÙˆØ¯ØŸ {'Ù†Ø¹Ù…' if m.get('disclaimer_present') else 'Ù„Ø§'} | Ø£Ù‚Ø³Ø§Ù… Ù†Ø§Ù‚ØµØ©: {', '.join(m.get('missing_sections', [])) or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}\n"
            f"- Ø¥Ø´Ø§Ø±Ø§Øª PAA: {m.get('paa_signals')}"
        )

        # ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ©
        actions = rep.get("suggested_actions", [])
        if actions:
            st.write("**ØªÙˆØµÙŠØ§Øª Ø¥ØµÙ„Ø§Ø­:**")
            for a in actions:
                st.write(f"â€¢ {a}")
        else:
            st.write("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØµÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ© â€” Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…ØªÙˆØ§Ø²Ù† ğŸ‘")

        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
        st.subheader("ğŸ“¤ ØªØµØ¯ÙŠØ±")
        md_bytes = to_markdown(result["article"]).encode("utf-8")
        docx_bytes = to_docx_bytes(result["article"], meta_title=meta.get("title", ""))
        json_bytes = to_json_bytes(
            article_markdown=result["article"],
            meta=meta,
            keyword=st.session_state["keyword"],
            related_keywords=st.session_state["related_keywords"],
        )

        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Markdown",
            data=md_bytes,
            file_name="article.md",
            mime="text/markdown"
        )
        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ DOCX",
            data=docx_bytes,
            file_name="article.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        st.download_button(
            "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ JSON",
            data=json_bytes,
            file_name="article.json",
            mime="application/json"
        )

    # ===== Ø§Ù„ÙÙ„ØªØ±Ø©/Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© =====
    st.markdown("---")
    st.subheader("ğŸ§¹ ÙÙ„ØªØ±Ø©/ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„ØºÙˆÙŠØ©")

    col_clean_a, col_clean_b = st.columns(2)
    with col_clean_a:
        opt_fix_punct = st.checkbox("ØªØµØ­ÙŠØ­ Ù…Ø³Ø§ÙØ§Øª/Ø´ÙƒÙ„ Ø§Ù„ØªØ±Ù‚ÙŠÙ…", value=True)
        opt_remove_filler = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø¬ÙÙ…Ù„/Ø£Ø³Ø·Ø± Ø­Ø´ÙˆÙŠØ©", value=True)
    with col_clean_b:
        opt_normalize_ws = st.checkbox("ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø±", value=True)
        opt_aggressive = st.checkbox("Ù†Ù…Ø· Ø¹Ø¯ÙˆØ§Ù†ÙŠ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø´Ùˆ (Ø­Ø°Ø±)", value=False)

    if st.button("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ù„ØºÙˆÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚"):
        with st.spinner("ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„..."):
            cleaned = clean_article(
                result["article"],
                remove_filler=opt_remove_filler,
                aggressive=opt_aggressive,
                fix_punct=opt_fix_punct,
                normalize_ws=opt_normalize_ws,
            )
            result["article"] = cleaned["cleaned"]
            st.session_state["result"] = result
            repc = cleaned["report"]
            st.success("ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")
            st.write("**ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ:**")
            st.write(
                f"- ØªØµØ­ÙŠØ­ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø°Ù: {repc['ellipsis_fixed']} | ØªÙ‚Ù„ÙŠØµ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {repc['repeated_punct_collapsed']}\n"
                f"- ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø³ØªØ¨Ø¯Ù„Ø©: {repc['arabic_comma_applied']} | Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§ÙØ§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {repc['spacing_fixed']}\n"
                f"- ØªØ·Ø¨ÙŠØ¹ Ù…Ø³Ø§ÙØ§Øª/Ø£Ø³Ø·Ø±: {repc['whitespace_normalized']} | Ø£Ø³Ø·Ø±/Ø¬Ù…Ù„ Ø­ÙØ°ÙØª/Ù‚ÙØµÙ‘Øª: {repc['filler_removed']}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:**")
            st.markdown(result["article"])

    # ===== Normalize Headings =====
    st.markdown("---")
    st.subheader("ğŸ§­ Normalize Headings (ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†)")

    col_h_a, col_h_b = st.columns(2)
    with col_h_a:
        opt_h1_to_h2 = st.checkbox("ØªØ­ÙˆÙŠÙ„ H1 Ø¥Ù„Ù‰ H2", value=True)
        opt_h4_to_h3 = st.checkbox("Ø¥Ù†Ø²Ø§Ù„ H4+ Ø¥Ù„Ù‰ H3", value=True)
        opt_space_after_hash = st.checkbox("ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨Ø¹Ø¯ #", value=True)
    with col_h_b:
        opt_trim_punct = st.checkbox("Ø¥Ø²Ø§Ù„Ø© ØªØ±Ù‚ÙŠÙ… Ø²Ø§Ø¦Ø¯ Ø¨Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", value=True)
        opt_collapse_spaces = st.checkbox("Ø¶ØºØ· Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", value=True)
        opt_dedupe = st.checkbox("Ø¥Ø²Ø§Ù„Ø© Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…ØªØªØ§Ù„ÙŠØ© Ù…ÙƒØ±Ø±Ø©", value=True)

    opt_autonumber = st.checkbox("ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø£Ù‚Ø³Ø§Ù… (H2/H3)", value=False)

    if st.button("ğŸ§­ Ø·Ø¨Ù‘Ù‚ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†"):
        with st.spinner("ØªØ·Ø¨ÙŠÙ‚ Normalize Headings..."):
            nh = normalize_headings(
                result["article"],
                h1_to_h2=opt_h1_to_h2,
                h4plus_to_h3=opt_h4_to_h3,
                unify_space_after_hash=opt_space_after_hash,
                trim_trailing_punct=opt_trim_punct,
                collapse_spaces=opt_collapse_spaces,
                remove_consecutive_duplicates=opt_dedupe,
                autonumber=opt_autonumber,
            )
            result["article"] = nh["normalized"]
            st.session_state["result"] = result

            ch = nh["changes"]
            st.success("ØªÙ… ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†.")
            st.write("**Ù…Ù„Ø®Øµ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª:**")
            st.write(
                f"- H1â†’H2: {ch['h1_to_h2']} | H4+â†’H3: {ch['h4plus_to_h3']} | Ù…Ø³Ø§ÙØ© Ø¨Ø¹Ø¯ #: {ch['space_after_hash_fixed']}\n"
                f"- Ø¥Ø²Ø§Ù„Ø© ØªØ±Ù‚ÙŠÙ… Ù†Ù‡Ø§Ø¦ÙŠ: {ch['trimmed_trailing_punct']} | Ø¶ØºØ· Ù…Ø³Ø§ÙØ§Øª: {ch['collapsed_spaces']} | Ø­Ø°Ù Ù…ÙƒØ±Ø±: {ch['deduped_headings']}\n"
                f"- ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ: {ch['autonumbered']} | Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†: {ch['total_headings']}"
            )
            st.markdown("**Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹:**")
            st.markdown(result["article"])

    # ===== Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© =====
    inventory = parse_inventory(inventory_raw)
    if inventory:
        st.subheader("ğŸ”— Ø§Ù‚ØªØ±Ø§Ø­ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©")
        suggestions = suggest_internal_links(
            keyword=st.session_state["keyword"],
            related_keywords=st.session_state["related_keywords"],
            article_markdown=result["article"],
            inventory=inventory,
            top_k=6,
        )
        if suggestions:
            for s in suggestions:
                st.markdown(f"- [{s['title']}]({s['url']}) â€” **Score:** {s['score']}")
        else:
            st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§Øª ÙƒØ§ÙÙŠØ© Ù…Ø¹ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ.")
    else:
        st.caption("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø®Ø²ÙˆÙ† Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©Ø› Ø£Ø¶ÙÙ JSON ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ Ø£Ø¹Ù„Ø§Ù‡ Ù„Ø±Ø¤ÙŠØ© Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª.")

    # ===== Ù…Ø¤Ø´Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ =====
    if previous_corpus_raw and previous_corpus_raw.strip():
        st.subheader("ğŸ­ Ù…Ø¤Ø´Ù‘Ø± ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨")
        srep = style_diversity_report(result["article"], previous_corpus_raw, top_k=5)

        st.write(f"**Ø­Ø¬Ù… ÙƒÙˆØ±Ø¨Ø³ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:** {srep.get('corpus_size')}")
        st.write(f"**Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Jaccard 3-grams):** {srep.get('avg_similarity')}")
        st.write(f"**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±:** {srep.get('risk_level')}")

        sm = srep.get("style_metrics", {})
        st.write(
            f"- TTR (ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª): {sm.get('type_token_ratio')}\n"
            f"- Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø© (ØªÙˆÙƒÙ†Ø²): {sm.get('avg_sentence_length_tokens')}\n"
            f"- ÙƒØ«Ø§ÙØ© Ø§Ù„ØªØ±Ù‚ÙŠÙ…: {sm.get('punctuation_density')} | Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„: {sm.get('sentences_count')} | Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²: {sm.get('tokens_count')}"
        )

        # Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªØ´Ø§Ø¨Ù‡Ù‹Ø§
        top_sim = srep.get("top_similar", [])
        if top_sim:
            st.write("**Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªØ´Ø§Ø¨Ù‡Ù‹Ø§:**")
            for i, item in enumerate(top_sim, 1):
                st.write(f"{i}. {item['title']} â€” similarity: {item['similarity']}")
        else:
            st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨Ø¯Ø±Ø¬Ø© Ù…Ù„Ø­ÙˆØ¸Ø©.")

        # ØªÙˆØµÙŠØ§Øª
        sugg = srep.get("suggestions", [])
        if sugg:
            st.write("**ØªÙˆØµÙŠØ§Øª ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡:**")
            for s in sugg:
                st.write(f"â€¢ {s}")
        else:
            st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØµÙŠØ§Øª â€” Ø§Ù„ØªÙ†ÙˆØ¹ Ø¬ÙŠØ¯ ğŸ‘")
    else:
        st.caption("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ù‚Ø§Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠØ©.")

    # ===== ØªØ­Ø±ÙŠØ± Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯ (Regenerate Section) =====
    st.markdown("---")
    st.subheader("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ù‚Ø³Ù… Ù…Ø­Ø¯Ø¯")

    secs = list_sections(result["article"])
    if not secs:
        st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ÙˆÙŠÙ† H2/H3 Ù„Ø§Ø®ØªÙŠØ§Ø± Ù‚Ø³Ù….")
    else:
        titles = [t for (t, lvl, s, e) in secs]
        selected = st.selectbox("Ø§Ø®ØªØ± Ù‚Ø³Ù…Ù‹Ø§ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯Ù‡:", titles, index=0)
        if selected:
            preview = extract_section_text(result["article"], selected)
            with st.expander("Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ"):
                st.markdown(preview)

            if st.button("ğŸ” Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…"):
                with st.spinner("ÙŠØ¹Ø§Ø¯ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯â€¦"):
                    try:
                        new_article = regenerate_section(
                            article_md=result["article"],
                            section_title=selected,
                            keyword=st.session_state["keyword"],
                            related_keywords=st.session_state["related_keywords"],
                            tone=st.session_state["tone"],
                        )
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„Ø¹Ø±Ø¶
                        result["article"] = new_article
                        st.session_state["result"] = result
                        st.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø³Ù… Ø¨Ù†Ø¬Ø§Ø­.")
                        st.markdown(new_article)
                    except Exception as e:
                        st.error(f"ØªØ¹Ø°Ù‘Ø±Øª Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù…: {e}")

    st.success("âœ… Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¬Ø§Ù‡Ø² â€” Ø£Ù†Ø´Ø¦ØŒ Ù†Ø¸Ù‘ÙØŒ Ø·Ø¨Ù‘Ø¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†ØŒ Ø§ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©ØŒ Ø§Ù‚ØªØ±Ø­ Ø±ÙˆØ§Ø¨Ø·ØŒ Ù‚ÙØ³ Ø§Ù„ØªÙ†ÙˆØ¹ØŒ ÙˆØ¹Ø¯Ù‘Ù„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ÙƒÙ…Ø§ ØªØ´Ø§Ø¡.")
